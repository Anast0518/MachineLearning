# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1npUOsZmYCkSvq_fO3xhZ50FahtGztY-y
"""

from sklearn.metrics import adjusted_rand_score, silhouette_score, homogeneity_score
from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_wine
from sklearn.cluster import KMeans, MeanShift, AffinityPropagation, DBSCAN, OPTICS
from sklearn.preprocessing import StandardScaler
import numpy
import warnings # убираем предупреждения
with warnings.catch_warnings():
    warnings.simplefilter("ignore")

"""***Homogeneity Score (Однородность кластеров)***:
измеряет связь между каждым кластером и классами данных, которые он содержит. Эта метрика позволяет оценить, насколько каждый кластер состоит из точек данных одного и того же класса. Значение однородности кластеров находится в диапазоне от 0 до 1.
    
    * 1: указывает на то, что каждый кластер содержит только точки данных одного класса.
     * 0:  Кластеры не содержат информации об истинном разбиении.
**Silhouette Score (Silhouette):**
измеряет сплоченность кластеров и разделяемость между ними.
Принцип:  Для каждого объекта вычисляет среднее расстояние до объектов в его кластере и до ближайшего другого кластера. Разница между этими расстояниями и нормализуется. Может находиться в диапазоне от -1 до 1.
    
    * 1:  Объект хорошо классифицирован, расположен близко к объектам своего кластера и далеко от других кластеров.
    * 0:  Объект находится на границе двух кластеров, не принадлежит ни к одному из них четко.
    * -1:  Объект неправильно классифицирован, находится ближе к объектам другого кластера.

 **Adjusted Rand Index (ARI):**
измеряет сходство между полученными кластерами и истинными метками.
Принцип: Вычисляет количество пар объектов, которые правильно классифицированы алгоритмом (в том же кластере, что и в истинном разбиении) и неправильно классифицированы. Может находиться в диапазоне от -1 до 1.

    * 1:  Полное совпадение между полученными и истинными кластерами.
    * 0:  Случайное разбиение на кластеры.
    * -1:  Полное несовпадение.

"""

datasets = numpy.array([load_iris(), load_breast_cancer(), load_digits(), load_wine()]) # датасеты
clustering_algorithms = numpy.array([KMeans, DBSCAN, OPTICS, MeanShift, AffinityPropagation]) # методы кластеризации
metrics =['euclidean', 'manhattan', 'chebyshev'] # метрики расстояния, которые используются в некоторых методах кластеризации
clustering_metrics = ['ARI', 'Silhouette', 'Homogeneity'] # метрики кластеризации
# цикл по датасетам
for dataset in datasets:
  print(dataset.DESCR.splitlines()[2]) # название датасета
  data = StandardScaler().fit_transform(dataset.data) # стандартизация данных
  target = dataset.target # выделяем метки
  # определяем количество уникальных значений в target, чтобы получить примерное количество кластеров
  range_clusters = [n for n in range(len(set(target)) - 1, len(set(target)) + 2) if n > 0] # создается список (кол-во кластеров), который варьируется вокруг уникальных значений в target
  best_algorithm = None
  best_param = None
  best_clus_metric = None
  best_score = float('-inf')

  # цикл по алгоритмам кластеризации
  for algorithm in clustering_algorithms:
        print('/t', algorithm) # название алгоритма кластеризации
        params = [None] # список параметров для каждого алгоритма
        # Если алгоритм KMeans, то параметр - количество кластеров
        if algorithm == KMeans:
          params = range_clusters
        elif algorithm in clustering_algorithms[1 : 3]:   # Если алгоритм DBSCAN, OPTICS
          params = metrics # то параметры - метрики расстояния
        # цикл по каждому параметру
        for param in params:
         if algorithm == KMeans:     # Если алгоритм - KMeans
            model = algorithm(n_clusters = param)
            print(f"\t\tn_clusters: {param} - ", end='')

         elif algorithm in clustering_algorithms[1 : 3]:   # Если алгоритмы DBSCAN, OPTICS
          model = algorithm(metric = param)
          print(f"\t\tmetric: {param} - ", end='')

         else:        # Если параметр отсутствует, то создается модель алгоритма с параметрами по умолчанию
            model = algorithm()
            print("\t\t", end='')

         predictions = model.fit_predict(data) # обучение модели и предсказывание кластеров для каждого объекта
         # расчет метрик кластеризации
         ari = adjusted_rand_score(target, predictions)
         silhouette = silhouette_score(data, predictions) if len(numpy.unique(predictions)) > 1 else numpy.nan
         homogeneity = homogeneity_score(target, predictions)

         scores = numpy.array([ari, silhouette, homogeneity]) # Массив, содержащий значения всех трех метрик кластеризации
         # сохранение лучших результатов
         if best_score < max(scores):
            best_algorithm = algorithm
            best_params = param
            best_metric = clustering_metrics[numpy.argmax(scores)]
            best_score = max(scores)
         print(f"ARI: {scores[0]:.4f}, Silhouette: {scores[1]:.4f}, Homogeneity: {scores[2]:.4f}") # значения всех трех метрик для текущей модели
        print()
  print("Лучший результат для этого датасета: \n")
  print(f"\t{best_algorithm}: {best_params} - {best_metric}: {best_score}\n") # информация о лучшем алгоритме, его параметре, метрике и значении
  print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n")