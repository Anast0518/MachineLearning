# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bmXMp97fUXjeDUAyzn1AJadYEoF1XNuL

Рубашевская Анастасия Андреевна 3.9. Лабораторная работа №1. Наивный Байесовский алгоритм
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # для рисования графиков
from sklearn import datasets # импорт датасетов
from sklearn.metrics import accuracy_score # импорт функции для нахождения точности
from sklearn.model_selection import train_test_split  # разделение датасетов
from sklearn.naive_bayes import GaussianNB, MultinomialNB # импорт нужных методов Байеса
from sklearn.utils import Bunch # импорт для создания экземпляров Bunch, которые используются для хранения данных датасета
from sklearn.preprocessing import OrdinalEncoder # нужно для кодировки категориальных датасетов

# Чтение датасета с моей фамилией
my_datasetname = pd.read_csv("Rubashevskaya.csv")
my_datasetname = my_datasetname.drop(columns = my_datasetname.columns[0]) # удаление первого столбца
Rubashevskaya_array = my_datasetname.to_numpy() # Перевод из csv в массив numpy
# Создание структуры своего датасета как в scklearn-е при помощи функции
def Rub_dataset():
  dataset = Bunch()
  # Проводим кодирование категориальных переменных. Чтобы строчные данные стали числами
  encoder = OrdinalEncoder() # Кодировка категориальных объектов в массив целых чисел.
  data_encoded = encoder.fit_transform(Rubashevskaya_array)
  # Делим датасет на данные и метки
  dataset.data = data_encoded[:, :-1] # Под данные выделяю все столбцы, кроме последнего
  dataset.target = data_encoded[:, -1] # Под метки выделяю последний столбец
  # Разделение данных нужно для того, чтобы модель могла учиться на признаках и предсказывать целевую переменную(метки)
  dataset.DESCR = ".. _Rub_dataset:\n\nФамильный датасет" # Название датасета
  dataset.target_names = np.unique(dataset.target) # Заношу в переменную уникальные значения меток
  return dataset
Rub_dataset()

class NB:
  def fit(self, X, y):
    self.classes = Rub_dataset().target_names # Присвоение уникальных значений в target (0,1,2) атрибуту self.classes
    # вычисление предполагаемых вероятностей для каждого класса. Сначала создаю пустой словарь для этого
    self.class_suggestion = {}
    # Считаем вероятности для каждого класса
    # Для каждого класса c мы считаем, сколько раз он встречается в y и делим на общее количество элементов в y
    for c in self.classes:
      self.class_suggestion[c] = np.sum(y == c) / len(y)
    self.sign_probs = {} # создание пустого словаря для вероятностей признаков каждого класса

    # Считаем вероятности признаков для каждого класса
    # Цикл по всем классам
    for c in self.classes:
      self.sign_probs[c] = []    # Создание пустого списка в self.sign_probs
      # Вложенный цикл по всем признакам (столбцам) в data
      for i in range(X.shape[1]):
        # Считаем вероятность, что данный признак  имеет уникальное значение для данного класса. Сохраняю в список sign_probs
        prob = {} # Создаю пустой словарь prob, который будет содержать вероятности для данного признака
        # Запускаю цикл по уникальным значениям данного признака
        for value in np.unique(X[:, i]):
          # Вычисляю вероятность того, что данный признак имеет значение value для класса c
          # Для этого считаю сколько раз это значение встречается в X и y одновременно, и делю на общее количество элементов в y для данного класса
          prob[value] = np.sum((X[:, i] == value) & (y == c)) / np.sum(y == c)
        self.sign_probs[c].append(prob)  # Добавляю словарь prob в список вероятностей признаков для данного класса

  def predict_proba(self, X):
    ''' Метод работает на основе предполагаемых значений вероятностей.
Вычисляет вероятности принадлежности каждого образца к каждому классу на основе предполагаемых вероятностей.
Использует формулу наивного байесовского классификатора для вычисления вероятностей для каждого класса.
Возвращает нормализованные вероятности для каждого класса.'''
    eps = 0.000001  # маленькое сглаживающее значение, которое используется для избегания деления на ноль
    # Создаю нулевой массив размером (количество образцов, количество классов) для хранения вероятностей
    probs = np.zeros((X.shape[0], len(self.classes)))
    # Запускаю цикл по всем классам
    for i, c in enumerate(self.classes):
      # Получаю вероятности признаков для текущего класса c
      sign_probs = self.sign_probs[c]
      # Создаю массив, заполненный преполагаемыми вероятностями для данного класса
      probs_class = np.full(X.shape[0], self.class_suggestion[c])
      # Запускаю вложенный цикл по всем признакам (столбцам) в матрице X
      for j in range(X.shape[1]):
        # Вычисляю вероятности для каждого признака в данном классе
        # Использую .get() для обработки отсутствующих значений (если они есть)
        probs_class *= [sign_probs[j].get(value, eps) for value in X[:, j]]
      # Сохраняю вероятности для данного класса в общий массив probs
      probs[:, i] = probs_class
    # Возвращаю нормализованные вероятности для каждого класса, поделив на сумму вероятностей по строкам
    return probs/np.sum(probs, axis=1, keepdims=True) # probs - из него берутся значения для суммирования, axis - задает, что суммирование будет построчное (0 - по столбцам)
  # keepdims - сохранение/несохранение размерности массива после суммирования

  def predict(self, X):
    '''Предсказывает класс для каждого образца X, выбирая класс с наибольшей вероятностью из predict_proba
    np.argmax() возвращает индекс первого встреченного элемента с максимальным значением'''
    # Предсказываю класс для каждого элемента в X, выбирая класс с максимальной вероятностью.
    return np.argmax(self.predict_proba(X), axis=1)

  def score(self, X, y):
    '''Возвращает долю правильных предсказаний модели
    np.mean() вычисляет среднее арифметическое'''
    # Вычисляю точность модели на данных X с известными метками классов y, сравнивая предсказанные классы с истинными.
    return np.mean(self.predict(X) == y)

data = Rub_dataset()
print("Название датасета: ",data.DESCR.splitlines()[2])    # атрибут DESCR содержит описание датасета, которое включает его имя.
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size = 0.7)
# train - тренировка, test - проверка; 70% - тренировка, оставшиеся 30% - тестирование
model = NB()          # обучаю модель
model.fit(X_train,y_train)
print("Точность, рассчитаная с помощью внутриклассового метода:", model.score(X_test, y_test)* 100,"%")
accuracy = accuracy_score(y_test, model.predict(X_test)) * 100
print("Точность предсказания вероятности по методу accuracy_score составляет",accuracy,"%\n")
print("-------------------------------------------------------------------------")

data_arr = [datasets.load_breast_cancer, datasets.load_iris, datasets.load_wine, datasets.load_digits]
print("Iris plants dataset - Набор данных о растениях ириса\nWine recognition dataset - Набор данных для распознавания вин\nBreast cancer wisconsin (diagnostic) dataset - Набор данных по раку молочной железы в Висконсине (диагностический)\nOptical recognition of handwritten digits dataset - Оптическое распознавание набора данных рукописных цифр")
print ("------------------------------------------------------------------------")
for dataset in data_arr:                     # Цикл прогона по 4 датасетам
  data = dataset()                              # Присвоение переменной data набор данных определенного датасета

  # атрибут DESCR - это строковая переменная или объект, которая, предположительно, содержит несколько строк текста.
  #.splitlines() - это метод строкового объекта, который разбивает строку на список строк, используя символы новой строки (перевода строки) в качестве разделителя.
  # [2] - это индексация списка, которая выбирает третью строку (от 0, поэтому [2] выберет третий элемент).
  # Результат заносится в переменную dataset_name и выводится
  dataset_name = data.DESCR.splitlines()[2]     # атрибут DESCR содержит описание датасета, которое включает его имя.
  print("Название датасета: ",dataset_name)
  # разделим данные с помощью train_test_split
  X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size = 0.7)
  # Нарисуем график расположения точек наших датасетов
  plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
  plt.xlabel('Признак 1')
  plt.ylabel('Признак 2')
  plt.title('Отображение точек на графике')
  plt.show()

data_arr = [datasets.load_breast_cancer, datasets.load_iris, datasets.load_wine]
print("Iris plants dataset - Набор данных о растениях ириса\nWine recognition dataset - Набор данных для распознавания вин\nBreast cancer wisconsin (diagnostic) dataset - Набор данных по раку молочной железы в Висконсине (диагностический)\nOptical recognition of handwritten digits dataset - Оптическое распознавание набора данных рукописных цифр")
print ("------------------------------------------------------------------------")
for dataset in data_arr:                     # Цикл прогона по 4 датасетам
  data = dataset()                              # Присвоение переменной data набор данных определенного датасета
  # атрибут DESCR - это строковая переменная или объект, которая, предположительно, содержит несколько строк текста.
  #.splitlines() - это метод строкового объекта, который разбивает строку на список строк, используя символы новой строки (перевода строки) в качестве разделителя.
  # [2] - это индексация списка, которая выбирает третью строку (от 0, поэтому [2] выберет третий элемент).
  # Результат заносится в переменную dataset_name и выводится
  dataset_name = data.DESCR.splitlines()[2]     # атрибут DESCR содержит описание датасета, которое включает его имя.
  print("Название датасета: ",dataset_name)
  # разделим данные с помощью train_test_split
  X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size = 0.7)
  model = GaussianNB()
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  print(f"Точность: {accuracy * 100} %")
# Датасету Optical recognition of handwritten digits dataset (цифры) подходит алгоритм "Мультиномиальный наивный Байес"
data_arr1 = datasets.load_digits()
print("Optical recognition of handwritten digits dataset",dataset_name)
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size = 0.7)
model = MultinomialNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Точность: {accuracy * 100} %")